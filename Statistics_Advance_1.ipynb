{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1.Explain the properties of the F-distribution.\n",
        "A-The F-distribution is a probability distribution that arises frequently in statistical tests, particularly in analyzing variances. Here are its key properties:\n",
        "\n",
        "Shape and Range: The F-distribution is positively skewed and only takes values from 0 to infinity. Its shape depends on the degrees of freedom of the numerator and the denominator. With larger sample sizes, it becomes less skewed and approaches a more normal distribution shape.\n",
        "\n",
        "Parameters (Degrees of Freedom): The F-distribution is defined by two parameters‚Äîdegrees of freedom for the numerator (\n",
        "ùëë\n",
        "1\n",
        "d\n",
        "1\n",
        "‚Äã\n",
        " ) and denominator (\n",
        "ùëë\n",
        "2\n",
        "d\n",
        "2\n",
        "‚Äã\n",
        " ). These degrees of freedom are typically based on the sample sizes from two different groups being compared.\n",
        "\n",
        "Asymmetry: The F-distribution is not symmetric; it is heavily skewed to the right. This skewness diminishes as the degrees of freedom increase, especially if both are high, making the distribution closer to normal.\n",
        "\n",
        "Non-Negative Values: All values in an F-distribution are non-negative. Since the F-distribution is used in variance comparisons, and variances are always non-negative, the distribution reflects this property.\n",
        "\n",
        "Use in Hypothesis Testing: The F-distribution is often used in the analysis of variance (ANOVA) and regression analysis. It helps in comparing variances across groups or models, which is useful in determining if differences observed between groups are statistically significant.\n",
        "\n",
        "The F-distribution is widely applied in ANOVA and F-tests because of its sensitivity to changes in variance, making it ideal for comparing variability across multiple groups or models."
      ],
      "metadata": {
        "id": "RYbDjjBV9xsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2.In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n",
        "A-The F-distribution is used in several statistical tests where comparing variances is essential. Here‚Äôs a look at the main types of tests and why the F-distribution is suitable for them:\n",
        "\n",
        "Analysis of Variance (ANOVA):\n",
        "\n",
        "ANOVA tests whether the means of three or more groups are significantly different. It does this by comparing the variance between group means to the variance within groups.\n",
        "The F-distribution is used here because it allows us to assess if the observed variance between groups is significantly larger than the variance within groups, suggesting true differences among group means rather than random variation.\n",
        "Regression Analysis:\n",
        "\n",
        "In multiple regression, the F-test helps determine whether the overall model fits the data better than a model with no predictors.\n",
        "By comparing the variance explained by the regression model to the residual (unexplained) variance, the F-test evaluates the model's significance. The F-distribution is appropriate because it can compare these variances effectively.\n",
        "Comparing Two Population Variances (F-test):\n",
        "\n",
        "An F-test is commonly used to compare the variances of two independent samples to test if they come from populations with equal variances.\n",
        "The F-distribution is suited here because it‚Äôs specifically designed to handle ratios of variances, making it a direct method for variance comparison.\n",
        "Why the F-Distribution is Appropriate\n",
        "The F-distribution is particularly useful in these tests because:\n",
        "\n",
        "It is sensitive to variance differences, which is crucial when comparing group variances or model variance.\n",
        "The distribution's right skew means it naturally accommodates larger values for the ratio of variances, making it effective for tests where we expect one variance to be larger if a significant difference exists.\n",
        "These properties make the F-distribution ideal for use in statistical tests where assessing variance relationships is fundamental."
      ],
      "metadata": {
        "id": "l7DF8Bsj93SJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3.In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n",
        "A-\n",
        "When conducting an F-test to compare the variances of two populations, several key assumptions must be met for the test to be valid:\n",
        "\n",
        "Normality:\n",
        "\n",
        "Both populations from which the samples are drawn should follow a normal distribution. The F-test is sensitive to deviations from normality, and non-normal data can lead to inaccurate results.\n",
        "Independence:\n",
        "\n",
        "The samples should be independent of each other. This means that the values in one sample should not influence or depend on the values in the other sample. Violations of independence can bias the test outcomes.\n",
        "Random Sampling:\n",
        "\n",
        "The samples should be randomly selected from the populations they represent. Random sampling helps ensure that the samples are representative of their respective populations.\n",
        "Homogeneity of Variances (for ANOVA using F-tests):\n",
        "\n",
        "Although the F-test is used to test for equal variances, in ANOVA, where F-tests are used to assess mean differences, an assumption of homogeneity of variances is required. This means that the variances across groups should be roughly equal. This assumption is also known as homoscedasticity.\n",
        "Scale of Measurement:\n",
        "\n",
        "The variables being compared should be measured on at least an interval scale, meaning that the differences between values are meaningful.\n",
        "Consequences of Violating Assumptions\n",
        "Violating these assumptions can lead to unreliable results:\n",
        "\n",
        "Non-normal data can make the F-statistic distribution inaccurate.\n",
        "Dependence between samples can inflate Type I error rates (incorrectly rejecting the null hypothesis).\n",
        "Non-random samples reduce the generalizability of results.\n",
        "Adjustments for Violations\n",
        "If these assumptions are not met, alternative tests like the Levene‚Äôs Test or Brown-Forsythe Test may be used, as they are less sensitive to normality violations when comparing variances."
      ],
      "metadata": {
        "id": "FDXX3WGq-x3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4.What is the purpose of ANOVA, and how does it differ from a t-test?\n",
        "A-\n",
        "The purpose of Analysis of Variance (ANOVA) is to determine if there are statistically significant differences between the means of three or more independent groups. While both ANOVA and the t-test can be used to compare group means, they differ in scope and application:\n",
        "\n",
        "Purpose of ANOVA:\n",
        "\n",
        "ANOVA is designed to test for mean differences across multiple groups simultaneously. By comparing the variance between groups to the variance within groups, ANOVA can determine if the observed differences in group means are likely due to random variation or represent true population differences.\n",
        "ANOVA yields an F-statistic, which indicates whether the variance between group means is significantly larger than the variance within the groups.\n",
        "Differences from the t-test:\n",
        "\n",
        "Number of Groups: A t-test compares the means of two groups, while ANOVA can compare three or more groups in one test.\n",
        "Type of Comparison: When comparing multiple groups, using t-tests repeatedly for every pair of groups increases the risk of Type I error (false positives). ANOVA controls this error by using a single test to compare all groups at once.\n",
        "Interpretation: ANOVA results indicate whether there is a significant difference among group means but do not specify which groups differ from each other. Post-hoc tests (such as Tukey‚Äôs HSD) are often needed after ANOVA to identify specific group differences.\n",
        "Applications:\n",
        "\n",
        "ANOVA is widely used in experimental and observational studies where comparisons among multiple groups are necessary, such as comparing treatment effects, regional differences, or different demographic categories.\n",
        "In summary, ANOVA is preferable when testing for differences among multiple groups because it provides a global assessment of differences while controlling the risk of Type I error, which can occur with multiple t-tests."
      ],
      "metadata": {
        "id": "euiAoO0F_LSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups.\n",
        "A-When comparing more than two groups, a one-way ANOVA is generally preferred over performing multiple t-tests for several reasons:\n",
        "\n",
        "Control of Type I Error:\n",
        "\n",
        "Each t-test carries a risk of Type I error (false positives). If multiple t-tests are performed to compare all possible pairs of groups, the overall chance of making at least one Type I error increases. This is known as the multiple comparisons problem.\n",
        "One-way ANOVA solves this by using a single test to evaluate all groups simultaneously, thus maintaining a fixed overall significance level (e.g., 5%).\n",
        "Efficiency:\n",
        "\n",
        "Conducting multiple t-tests is time-consuming and inefficient, especially as the number of groups increases. For example, with five groups, ten pairwise t-tests are needed, while ANOVA requires only one test, regardless of the number of groups.\n",
        "Interpretation:\n",
        "\n",
        "One-way ANOVA provides a single F-statistic and p-value that indicate whether there is at least one significant difference between the group means. This streamlined approach makes it easier to determine if differences exist among groups without having to interpret multiple test results.\n",
        "If ANOVA indicates significant differences, post-hoc tests (like Tukey‚Äôs HSD) can then identify specific pairs of groups with statistically significant differences, without increasing the Type I error rate.\n",
        "Application to Larger Datasets:\n",
        "\n",
        "One-way ANOVA is well-suited for datasets with multiple groups and larger sample sizes. It is often used in fields like psychology, biology, and social sciences, where researchers compare several treatments or groups.\n",
        "Summary\n",
        "In cases where you need to compare more than two groups, one-way ANOVA is the preferred method because it provides a single, reliable test that controls for Type I error and allows for more efficient analysis.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VSzfELRC_gck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
        "How does this partitioning contribute to the calculation of the F-statistic?\n",
        "A-In Analysis of Variance (ANOVA), variance is partitioned into two components: between-group variance and within-group variance. This partitioning helps in determining whether there are significant differences among group means by comparing the variability between groups to the variability within the groups.\n",
        "\n",
        "1. Between-Group Variance (or Explained Variance)\n",
        "This refers to the variance due to the differences between the group means. It measures how much the group means differ from the overall mean (grand mean).\n",
        "The formula for between-group variance is calculated by the sum of squares between groups (SSB), divided by the degrees of freedom between groups (df_between).\n",
        "The formula for the sum of squares between (SSB) is:\n",
        "\n",
        "ùëÜ\n",
        "ùëÜ\n",
        "ùêµ\n",
        "=\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëò\n",
        "ùëõ\n",
        "ùëñ\n",
        "(\n",
        "ùëã\n",
        "Àâ\n",
        "ùëñ\n",
        "‚àí\n",
        "ùëã\n",
        "Àâ\n",
        ")\n",
        "2\n",
        "SSB=\n",
        "i=1\n",
        "‚àë\n",
        "k\n",
        "‚Äã\n",
        " n\n",
        "i\n",
        "‚Äã\n",
        " (\n",
        "X\n",
        "Àâ\n",
        "  \n",
        "i\n",
        "‚Äã\n",
        " ‚àí\n",
        "X\n",
        "Àâ\n",
        " )\n",
        "2\n",
        "\n",
        "Where:\n",
        "\n",
        "ùëò\n",
        "k is the number of groups\n",
        "ùëõ\n",
        "ùëñ\n",
        "n\n",
        "i\n",
        "‚Äã\n",
        "  is the number of observations in group\n",
        "ùëñ\n",
        "i\n",
        "ùëã\n",
        "Àâ\n",
        "ùëñ\n",
        "X\n",
        "Àâ\n",
        "  \n",
        "i\n",
        "‚Äã\n",
        "  is the mean of group\n",
        "ùëñ\n",
        "i\n",
        "ùëã\n",
        "Àâ\n",
        "X\n",
        "Àâ\n",
        "  is the grand mean (the mean of all data points)\n",
        "The mean square between groups (MSB) is obtained by dividing the SSB by the degrees of freedom between groups, which is\n",
        "ùëë\n",
        "ùëì\n",
        "ùëè\n",
        "ùëí\n",
        "ùë°\n",
        "ùë§\n",
        "ùëí\n",
        "ùëí\n",
        "ùëõ\n",
        "=\n",
        "ùëò\n",
        "‚àí\n",
        "1\n",
        "df\n",
        "between\n",
        "‚Äã\n",
        " =k‚àí1.\n",
        "\n",
        "ùëÄ\n",
        "ùëÜ\n",
        "ùêµ\n",
        "=\n",
        "ùëÜ\n",
        "ùëÜ\n",
        "ùêµ\n",
        "ùëë\n",
        "ùëì\n",
        "ùëè\n",
        "ùëí\n",
        "ùë°\n",
        "ùë§\n",
        "ùëí\n",
        "ùëí\n",
        "ùëõ\n",
        "MSB=\n",
        "df\n",
        "between\n",
        "‚Äã\n",
        "\n",
        "SSB\n",
        "‚Äã\n",
        "\n",
        "2. Within-Group Variance (or Unexplained Variance)\n",
        "This is the variance within each group, reflecting the variability of individual observations around their group mean. It accounts for the natural variation within each group.\n",
        "The formula for within-group variance is calculated by the sum of squares within groups (SSW), divided by the degrees of freedom within groups (df_within).\n",
        "The formula for the sum of squares within (SSW) is:\n",
        "\n",
        "ùëÜ\n",
        "ùëÜ\n",
        "ùëä\n",
        "=\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëò\n",
        "‚àë\n",
        "ùëó\n",
        "=\n",
        "1\n",
        "ùëõ\n",
        "ùëñ\n",
        "(\n",
        "ùëã\n",
        "ùëñ\n",
        "ùëó\n",
        "‚àí\n",
        "ùëã\n",
        "Àâ\n",
        "ùëñ\n",
        ")\n",
        "2\n",
        "SSW=\n",
        "i=1\n",
        "‚àë\n",
        "k\n",
        "‚Äã\n",
        "  \n",
        "j=1\n",
        "‚àë\n",
        "n\n",
        "i\n",
        "‚Äã\n",
        "\n",
        "‚Äã\n",
        " (X\n",
        "ij\n",
        "‚Äã\n",
        " ‚àí\n",
        "X\n",
        "Àâ\n",
        "  \n",
        "i\n",
        "‚Äã\n",
        " )\n",
        "2\n",
        "\n",
        "Where:\n",
        "\n",
        "ùëã\n",
        "ùëñ\n",
        "ùëó\n",
        "X\n",
        "ij\n",
        "‚Äã\n",
        "  is an individual data point in group\n",
        "ùëñ\n",
        "i\n",
        "ùëã\n",
        "Àâ\n",
        "ùëñ\n",
        "X\n",
        "Àâ\n",
        "  \n",
        "i\n",
        "‚Äã\n",
        "  is the mean of group\n",
        "ùëñ\n",
        "i\n",
        "The mean square within groups (MSW) is obtained by dividing the SSW by the degrees of freedom within groups, which is\n",
        "ùëë\n",
        "ùëì\n",
        "ùë§\n",
        "ùëñ\n",
        "ùë°\n",
        "‚Ñé\n",
        "ùëñ\n",
        "ùëõ\n",
        "=\n",
        "ùëÅ\n",
        "‚àí\n",
        "ùëò\n",
        "df\n",
        "within\n",
        "‚Äã\n",
        " =N‚àík, where\n",
        "ùëÅ\n",
        "N is the total number of observations across all groups.\n",
        "\n",
        "ùëÄ\n",
        "ùëÜ\n",
        "ùëä\n",
        "=\n",
        "ùëÜ\n",
        "ùëÜ\n",
        "ùëä\n",
        "ùëë\n",
        "ùëì\n",
        "ùë§\n",
        "ùëñ\n",
        "ùë°\n",
        "‚Ñé\n",
        "ùëñ\n",
        "ùëõ\n",
        "MSW=\n",
        "df\n",
        "within\n",
        "‚Äã\n",
        "\n",
        "SSW\n",
        "‚Äã\n",
        "\n",
        "3. F-Statistic Calculation\n",
        "The F-statistic is used to test whether the group means are significantly different. It is the ratio of the between-group variance to the within-group variance:\n",
        "\n",
        "ùêπ\n",
        "=\n",
        "ùëÄ\n",
        "ùëÜ\n",
        "ùêµ\n",
        "ùëÄ\n",
        "ùëÜ\n",
        "ùëä\n",
        "F=\n",
        "MSW\n",
        "MSB\n",
        "‚Äã\n",
        "\n",
        "Where:\n",
        "\n",
        "MSB is the mean square between groups (i.e., the between-group variance)\n",
        "MSW is the mean square within groups (i.e., the within-group variance)\n",
        "Interpretation:\n",
        "If the group means are similar, the between-group variance (MSB) will be small, and the within-group variance (MSW) will dominate, resulting in a small F-statistic.\n",
        "If the group means are different, the between-group variance (MSB) will be large, and the F-statistic will be large.\n",
        "The F-statistic is then compared to a critical value from the F-distribution table to determine if the result is statistically significant. A larger F-statistic indicates that the differences between group means are likely not due to random chance.\n",
        "In summary, partitioning the total variance into between-group and within-group components helps assess whether the observed group differences are greater than what would be expected due to random variation within the groups. The F-statistic quantifies this ratio, and a significant F-statistic suggests that at least one group mean is different from the others."
      ],
      "metadata": {
        "id": "Boc_pAUyACyk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7.Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n",
        "differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?\n",
        "A-The classical (frequentist) approach and the Bayesian approach to ANOVA (Analysis of Variance) differ significantly in terms of handling uncertainty, parameter estimation, and hypothesis testing. Below are the key differences:\n",
        "\n",
        "1. Handling Uncertainty\n",
        "Frequentist Approach:\n",
        "In the classical framework, uncertainty is managed by considering data as coming from a fixed, unknown population. The true parameters (like the population means) are fixed but unknown.\n",
        "Uncertainty is quantified using confidence intervals and p-values, which provide probabilistic statements about the data, assuming repeated sampling. The focus is on how likely the observed data would be under different parameter values (e.g., assuming the null hypothesis is true).\n",
        "Bayesian Approach:\n",
        "In the Bayesian framework, uncertainty is treated as a distribution over possible parameter values. Parameters are not fixed but are modeled as random variables with prior distributions that reflect prior knowledge or beliefs about the parameters.\n",
        "Uncertainty is quantified through the posterior distribution, which updates the prior beliefs about the parameters after observing the data. The posterior combines prior information and the likelihood of the observed data.\n",
        "2. Parameter Estimation\n",
        "Frequentist Approach:\n",
        "\n",
        "Parameters are estimated using point estimates (such as sample means) and tested with statistical tests (e.g., F-tests). The frequentist method focuses on finding the \"best\" estimate of the parameter based on the data, with confidence intervals providing a range of plausible values for the parameter.\n",
        "In ANOVA, for instance, we estimate group means and the variance components (between-group variance and within-group variance). The null hypothesis is that all group means are equal, and this is tested using the F-statistic.\n",
        "Bayesian Approach:\n",
        "\n",
        "Parameters are treated as random variables with probability distributions, rather than fixed quantities. Bayesian estimation involves updating prior beliefs with new data to obtain a posterior distribution.\n",
        "In Bayesian ANOVA, the parameters of interest (such as group means or variance components) are estimated by calculating the posterior distribution of the parameters given the data. This provides a more comprehensive view of uncertainty, not just a point estimate.\n",
        "The Bayesian approach can give credible intervals, which represent the range of values within which the parameter lies with a certain probability, offering more flexibility in quantifying uncertainty.\n",
        "3. Hypothesis Testing\n",
        "Frequentist Approach:\n",
        "Hypothesis testing is based on null hypothesis significance testing (NHST). In ANOVA, the null hypothesis typically states that all group means are equal (no significant effect), and the alternative hypothesis suggests that at least one group mean differs.\n",
        "The test is done using an F-statistic to calculate a p-value. The p-value indicates the probability of obtaining results at least as extreme as the observed results, assuming the null hypothesis is true. A small p-value (typically < 0.05) leads to rejecting the null hypothesis.\n",
        "Bayesian Approach:\n",
        "In the Bayesian approach, hypothesis testing is framed in terms of Bayes factors, which compare the evidence for one hypothesis over another.\n",
        "Instead of p-values, Bayesian testing evaluates the posterior odds of hypotheses or models. A Bayes factor quantifies how much more likely the data is under one hypothesis compared to another. This approach allows for more direct comparison between models or hypotheses.\n",
        "The Bayesian approach does not rely on rejecting or failing to reject a null hypothesis. Instead, it focuses on updating beliefs and evaluating the strength of evidence for different hypotheses.\n",
        "4. Interpretation of Results\n",
        "Frequentist Approach:\n",
        "\n",
        "Results are interpreted in terms of long-run frequencies and sampling distributions. For example, a 95% confidence interval means that if the experiment were repeated many times, 95% of the intervals would contain the true parameter value.\n",
        "A significant F-test in ANOVA implies that there is strong evidence against the null hypothesis, but it does not directly tell you the probability that the null hypothesis is true.\n",
        "Bayesian Approach:\n",
        "\n",
        "Results are interpreted in terms of probabilities of parameters or hypotheses. For example, a Bayesian credible interval gives the probability that the true parameter lies within that interval, given the data and prior information.\n",
        "A Bayesian hypothesis test provides the posterior probability of hypotheses, directly addressing the probability of different models or parameter values given the data.\n",
        "5. Flexibility in Model Building\n",
        "Frequentist Approach:\n",
        "\n",
        "The frequentist approach typically requires a pre-specified model, such as assuming the group means follow a normal distribution and that variances are equal in all groups (homogeneity of variance assumption).\n",
        "There are limitations in dealing with complex or hierarchical models (e.g., mixed-effects models) in a straightforward manner, although methods like ANOVA can be extended to more complex designs.\n",
        "Bayesian Approach:\n",
        "\n",
        "The Bayesian approach is more flexible in terms of model specification. It allows for more complex models, including hierarchical models and models with prior distributions tailored to specific contexts.\n",
        "Bayesian methods can also more naturally incorporate missing data, random effects, and non-standard likelihoods, providing a more flexible approach to real-world data.\n",
        "Summary Table: Key Differences\n",
        "Feature\tClassical (Frequentist) Approach\tBayesian Approach\n",
        "Uncertainty\tQuantified with confidence intervals and p-values\tQuantified with posterior distributions\n",
        "Parameter Estimation\tPoint estimates (e.g., sample means)\tParameter distributions (e.g., posterior distributions)\n",
        "Hypothesis Testing\tp-value, F-test (null hypothesis testing)\tBayes factors, posterior odds\n",
        "Interpretation\tLong-run frequencies, confidence intervals\tProbabilities for parameters and hypotheses\n",
        "Model Flexibility\tLess flexible, requires pre-specified models\tMore flexible, can incorporate complex models and prior knowledge\n",
        "Conclusion:\n",
        "The frequentist approach to ANOVA focuses on hypothesis testing based on a fixed model, with uncertainty captured through confidence intervals and p-values. The Bayesian approach, on the other hand, offers a more flexible and nuanced treatment of uncertainty, with parameter estimates represented as distributions and hypothesis testing framed in terms of posterior probabilities and Bayes factors. The Bayesian approach allows for a more comprehensive treatment of uncertainty and model complexity, while the frequentist approach is more straightforward and widely used for simpler problems."
      ],
      "metadata": {
        "id": "Z9kjwcPDAbNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q8.\n",
        "import numpy as np\n",
        "from scipy.stats import f\n",
        "\n",
        "\n",
        "profession_A = np.array([48, 52, 55, 60, 62])\n",
        "profession_B = np.array([45, 50, 55, 52, 47])\n",
        "\n",
        "\n",
        "var_A = np.var(profession_A, ddof=1)\n",
        "var_B = np.var(profession_B, ddof=1)\n",
        "\n",
        "\n",
        "F_stat = var_A / var_B if var_A > var_B else var_B / var_A\n",
        "\n",
        "\n",
        "df1 = len(profession_A) - 1\n",
        "df2 = len(profession_B) - 1\n",
        "\n",
        "\n",
        "p_value = 2 * min(f.cdf(F_stat, df1, df2), 1 - f.cdf(F_stat, df1, df2))\n",
        "\n",
        "print(\"F-statistic:\", F_stat)\n",
        "print(\"p-value:\", p_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC005EvbNgwd",
        "outputId": "271b457e-c00f-499a-cc1f-6791d61512b1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 2.089171974522293\n",
            "p-value: 0.49304859900533904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q9.\n",
        "from scipy.stats import f_oneway\n",
        "\n",
        "region_A = [160, 162, 165, 158, 164]\n",
        "region_B = [172, 175, 170, 168, 174]\n",
        "region_C = [180, 182, 179, 185, 183]\n",
        "\n",
        "\n",
        "F_stat, p_value = f_oneway(region_A, region_B, region_C)\n",
        "\n",
        "\n",
        "print(\"F-statistic:\", F_stat)\n",
        "print(\"p-value:\", p_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GI_vRGYlOG4-",
        "outputId": "83419edb-0567-4437-a28a-91b83babc2a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.87330316742101\n",
            "p-value: 2.870664187937026e-07\n"
          ]
        }
      ]
    }
  ]
}